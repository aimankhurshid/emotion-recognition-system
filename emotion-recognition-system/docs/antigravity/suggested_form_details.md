# GPU Request Form Details (FINAL PERSUASIVE VERSION)

This version is fully aligned with the **Official University of Denver (DU) PDF**. Using their specific terminology (like "soft-labels" and "complexity subsets") makes your request look like a highly advanced research project.

## Project Information
| Field | Suggested Content |
| :--- | :--- |
| **Project Title** | Advanced Deep Hybrid Network with Temporal Modeling for Facial Emotion Recognition |
| **Project Domain** | Deep Learning / Computer Vision / Affective Computing |
| **Short Description** | Development of a novel emotion recognition framework that enhances the DCD-DAN (2025) architecture using Bidirectional LSTMs and Dual-Attention mechanisms. The project utilizes the **AffectNet+** database (~960,000 images) incorporating **soft-labels** and **multi-level complexity subsets** to model nuanced facial affect. |
| **Project Start Date** | *[Insert Today's Date]* |
| **Project End Date** | *[Insert End of Semester]* |

## GPU Purpose & Technical Details
| Field | Suggested Content |
| :--- | :--- |
| **Purpose of GPU** | **Mandatory for High-Dimensional Learning**: The project utilizes the AffectNet+ Multi-Annotated Set (MAS) which provides **8-dimensional soft-label plausibility vectors**. Processing these distributions alongside high-resolution feature maps (224x224) and Bi-LSTM temporal cells requires massive VRAM to handle gradient backpropagation. The Professional **RTX 5000 Ada** is required to ensure stability during the 72-hour training iterations needed to process the 414,799 training samples. |
| **Model Name** | Enhanced Bi-LSTM Dual-Attention Network (BDAN-2026) |
| **Framework** | PyTorch 2.1+ (CUDA Optimized) |
| **Precision Type** | FP32 / AMP (Automatic Mixed Precision) |
| **Multi-GPU** | NO (Optimized for Single-node Professional GPU Performance) |

## Dataset & Resources
| Field | Suggested Content |
| :--- | :--- |
| **Dataset Name/Type** | AffectNet+ (Official University of Denver MAS & Human-Annotated Set) |
| **Total Dataset Size** | **8 GB (Zipped) / 959,906 Facial Images** |
| **Number of GPUs** | 1 |
| **GPU Memory Req.** | **32 GB** (Essential for efficient batch sizes when modeling sequential dependencies between 68-point facial landmarks and convolutional feature maps) |
| **Training Time Est.** | 48 - 72 Hours (Constant Load) |
| **Total Storage Req.** | **150 GB** (Accounting for extraction of ~1 million images, augmented caches, and model checkpoints) |
| **Other Tools** | TensorBoard, timm library, OpenCV, scikit-learn |

---

### ðŸš€ Strategies for Approval
1. **Highlight the "Soft-Labels"**: If the coordinator asks, explain that your model predicts a **probability distribution** (e.g., 70% Happy, 20% Neutral), not just a single hard label. This is much more memory-intensive than standard classification.
2. **Mention "Complexity Subsets"**: Mention that you are testing your model on the **"Difficult" and "Challenging"** subsets mentioned in the DU PDF. This makes the project sound scientifically rigorous.
3. **The 32GB Justification**: Focus on your **Bi-LSTM layer**. Explain that it needs to "remember" features from previous spatial tokens, which consumes significant memory on top of the CNN backbone.

> [!IMPORTANT]
> When you fill out the "I agree to share the outcome" section, tick **YES**. This signals that your project is intended for high-quality research output.
