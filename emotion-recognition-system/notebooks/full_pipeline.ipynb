{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Learning Emotion Recognition System - Complete Pipeline\n",
                "\n",
                "This notebook demonstrates the complete workflow:\n",
                "1. Environment setup\n",
                "2. Data exploration\n",
                "3. Model architecture overview\n",
                "4. Training demonstration\n",
                "5. Evaluation and metrics\n",
                "6. Inference examples\n",
                "7. Ablation study results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.append('..')\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "import cv2\n",
                "\n",
                "from models import get_model, EMOTION_LABELS\n",
                "from utils import get_data_loaders, plot_confusion_matrix, compute_metrics\n",
                "\n",
                "%matplotlib inline\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_DIR = '../data'\n",
                "\n",
                "if os.path.exists(DATA_DIR):\n",
                "    train_loader, val_loader, test_loader, class_weights = get_data_loaders(\n",
                "        DATA_DIR, batch_size=16, num_workers=2\n",
                "    )\n",
                "    \n",
                "    print(f\"Dataset loaded successfully!\")\n",
                "    print(f\"Training batches: {len(train_loader)}\")\n",
                "    print(f\"Validation batches: {len(val_loader)}\")\n",
                "    print(f\"Test batches: {len(test_loader)}\")\n",
                "else:\n",
                "    print(\"⚠️ Dataset not found. Please download AffectNet+ and place in data/ directory\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize class weights\n",
                "if 'class_weights' in locals() and class_weights is not None:\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.bar(EMOTION_LABELS, class_weights, color='skyblue', edgecolor='navy')\n",
                "    plt.xlabel('Emotion Class', fontsize=12)\n",
                "    plt.ylabel('Class Weight', fontsize=12)\n",
                "    plt.title('Class Weights for Handling Imbalanced Data', fontsize=14, fontweight='bold')\n",
                "    plt.xticks(rotation=45)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sample images\n",
                "if 'train_loader' in locals():\n",
                "    images, labels = next(iter(train_loader))\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "    axes = axes.flatten()\n",
                "    \n",
                "    for i in range(min(8, len(images))):\n",
                "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
                "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
                "        img = np.clip(img, 0, 1)\n",
                "        \n",
                "        axes[i].imshow(img)\n",
                "        axes[i].set_title(f\"Label: {EMOTION_LABELS[labels[i]]}\", fontsize=12, fontweight='bold')\n",
                "        axes[i].axis('off')\n",
                "    \n",
                "    plt.suptitle('Sample Training Images', fontsize=16, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Architecture Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create full model\n",
                "model = get_model(\n",
                "    model_type='full',\n",
                "    num_classes=8,\n",
                "    backbone='efficientnet_b4',\n",
                "    pretrained=True,\n",
                "    lstm_hidden=256,\n",
                "    lstm_layers=2,\n",
                "    dropout=0.5\n",
                ")\n",
                "\n",
                "model = model.to(device)\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"MODEL ARCHITECTURE\")\n",
                "print(\"=\"*80)\n",
                "print(model)\n",
                "print(\"=\"*80)\n",
                "print(f\"Total parameters: {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")\n",
                "print(f\"Model size: ~{total_params * 4 / 1e6:.2f} MB\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test forward pass\n",
                "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    output = model(dummy_input)\n",
                "    probs = torch.softmax(output, dim=1)\n",
                "\n",
                "print(f\"Input shape: {dummy_input.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Output probabilities sum: {probs.sum().item():.4f}\")\n",
                "print(f\"\\nSample output probabilities:\")\n",
                "for i, (emotion, prob) in enumerate(zip(EMOTION_LABELS, probs[0])):\n",
                "    print(f\"  {emotion:<12}: {prob.item()*100:>6.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Demonstration\n",
                "\n",
                "For demonstration, we'll train for just 2 epochs. For full training, use the `train.py` script."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from models import WeightedCrossEntropyLoss\n",
                "from utils import AverageMeter\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Setup training (demo only - 2 epochs)\n",
                "if 'train_loader' in locals():\n",
                "    criterion = WeightedCrossEntropyLoss(class_weights, device=device)\n",
                "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
                "    \n",
                "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "    \n",
                "    print(\"Starting demo training (2 epochs)...\")\n",
                "    print(\"For full training, use: python training/train.py --epochs 50\")\n",
                "    \n",
                "    for epoch in range(1, 3):  # Demo: only 2 epochs\n",
                "        # Training\n",
                "        model.train()\n",
                "        losses = AverageMeter()\n",
                "        accuracies = AverageMeter()\n",
                "        \n",
                "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch} [Train]'):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            _, predicted = outputs.max(1)\n",
                "            accuracy = (predicted == labels).float().mean().item() * 100\n",
                "            \n",
                "            losses.update(loss.item(), images.size(0))\n",
                "            accuracies.update(accuracy, images.size(0))\n",
                "        \n",
                "        history['train_loss'].append(losses.avg)\n",
                "        history['train_acc'].append(accuracies.avg)\n",
                "        \n",
                "        # Validation\n",
                "        model.eval()\n",
                "        val_losses = AverageMeter()\n",
                "        val_accuracies = AverageMeter()\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for images, labels in tqdm(val_loader, desc=f'Epoch {epoch} [Val]'):\n",
                "                images, labels = images.to(device), labels.to(device)\n",
                "                \n",
                "                outputs = model(images)\n",
                "                loss = criterion(outputs, labels)\n",
                "                \n",
                "                _, predicted = outputs.max(1)\n",
                "                accuracy = (predicted == labels).float().mean().item() * 100\n",
                "                \n",
                "                val_losses.update(loss.item(), images.size(0))\n",
                "                val_accuracies.update(accuracy, images.size(0))\n",
                "        \n",
                "        history['val_loss'].append(val_losses.avg)\n",
                "        history['val_acc'].append(val_accuracies.avg)\n",
                "        \n",
                "        print(f\"\\nEpoch {epoch}/2:\")\n",
                "        print(f\"  Train Loss: {losses.avg:.4f}, Train Acc: {accuracies.avg:.2f}%\")\n",
                "        print(f\"  Val Loss: {val_losses.avg:.4f}, Val Acc: {val_accuracies.avg:.2f}%\")\n",
                "else:\n",
                "    print(\"⚠️ Skipping training demo - dataset not loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "if 'history' in locals():\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "    \n",
                "    ax1.plot(history['train_loss'], 'b-', label='Training Loss', linewidth=2, marker='o')\n",
                "    ax1.plot(history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s')\n",
                "    ax1.set_title('Loss Curve', fontsize=14, fontweight='bold')\n",
                "    ax1.set_xlabel('Epoch')\n",
                "    ax1.set_ylabel('Loss')\n",
                "    ax1.legend()\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    ax2.plot(history['train_acc'], 'b-', label='Training Accuracy', linewidth=2, marker='o')\n",
                "    ax2.plot(history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2, marker='s')\n",
                "    ax2.set_title('Accuracy Curve', fontsize=14, fontweight='bold')\n",
                "    ax2.set_xlabel('Epoch')\n",
                "    ax2.set_ylabel('Accuracy (%)')\n",
                "    ax2.legend()\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation and Metrics\n",
                "\n",
                "After full training, evaluate on test set. Here we'll demonstrate with validation set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on validation set\n",
                "if 'val_loader' in locals():\n",
                "    model.eval()\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(val_loader, desc='Evaluating'):\n",
                "            images = images.to(device)\n",
                "            outputs = model(images)\n",
                "            _, predicted = outputs.max(1)\n",
                "            \n",
                "            all_preds.extend(predicted.cpu().numpy())\n",
                "            all_labels.extend(labels.numpy())\n",
                "    \n",
                "    all_preds = np.array(all_preds)\n",
                "    all_labels = np.array(all_labels)\n",
                "    \n",
                "    metrics = compute_metrics(all_labels, all_preds, average='macro')\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"EVALUATION RESULTS (Validation Set)\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"Accuracy:  {metrics['accuracy']*100:.2f}%\")\n",
                "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
                "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
                "    print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
                "    print(\"=\"*60)\n",
                "else:\n",
                "    print(\"⚠️ Skipping evaluation - dataset not loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "if 'all_preds' in locals():\n",
                "    from sklearn.metrics import confusion_matrix\n",
                "    \n",
                "    cm = confusion_matrix(all_labels, all_preds)\n",
                "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "    \n",
                "    plt.figure(figsize=(12, 10))\n",
                "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
                "                xticklabels=EMOTION_LABELS, yticklabels=EMOTION_LABELS,\n",
                "                cbar_kws={'label': 'Proportion'})\n",
                "    plt.title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')\n",
                "    plt.ylabel('True Label', fontsize=12)\n",
                "    plt.xlabel('Predicted Label', fontsize=12)\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict on random validation samples\n",
                "if 'val_loader' in locals():\n",
                "    images, labels = next(iter(val_loader))\n",
                "    images = images.to(device)\n",
                "    \n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        outputs = model(images)\n",
                "        probs = torch.softmax(outputs, dim=1)\n",
                "        confidences, predicted = probs.max(1)\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
                "    axes = axes.flatten()\n",
                "    \n",
                "    for i in range(min(8, len(images))):\n",
                "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
                "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
                "        img = np.clip(img, 0, 1)\n",
                "        \n",
                "        true_label = EMOTION_LABELS[labels[i]]\n",
                "        pred_label = EMOTION_LABELS[predicted[i]]\n",
                "        confidence = confidences[i].item()\n",
                "        \n",
                "        color = 'green' if labels[i] == predicted[i] else 'red'\n",
                "        \n",
                "        axes[i].imshow(img)\n",
                "        axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence*100:.1f}%\",\n",
                "                         fontsize=11, fontweight='bold', color=color)\n",
                "        axes[i].axis('off')\n",
                "    \n",
                "    plt.suptitle('Sample Predictions', fontsize=16, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"⚠️ Skipping inference demo - dataset not loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Ablation Study Results\n",
                "\n",
                "After running ablation study, results will be available here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load ablation results if available\n",
                "ablation_results_path = '../results/ablation/ablation_results.csv'\n",
                "\n",
                "if os.path.exists(ablation_results_path):\n",
                "    df_ablation = pd.read_csv(ablation_results_path)\n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"ABLATION STUDY RESULTS\")\n",
                "    print(\"=\"*80)\n",
                "    print(df_ablation.to_string(index=False))\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Visualize ablation results\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    x = np.arange(len(df_ablation))\n",
                "    width = 0.6\n",
                "    \n",
                "    accuracies = [float(acc.strip('%')) for acc in df_ablation['Accuracy (%)']]\n",
                "    \n",
                "    bars = ax.bar(x, accuracies, width, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
                "    \n",
                "    ax.set_xlabel('Model Configuration', fontsize=12)\n",
                "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
                "    ax.set_title('Ablation Study - Impact of Architecture Components', fontsize=14, fontweight='bold')\n",
                "    ax.set_xticks(x)\n",
                "    ax.set_xticklabels(df_ablation['Configuration'], rotation=15, ha='right')\n",
                "    ax.grid(True, alpha=0.3, axis='y')\n",
                "    \n",
                "    for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "               f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"⚠️ Ablation results not found. Run: python training/ablation_study.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Comparison with Base Paper"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "comparison_data = {\n",
                "    'Model': ['DCD-DAN (2025)', 'Our Model (Proposed)'],\n",
                "    'Architecture': ['CNN + Cross-Domain Dual Attention', 'CNN + Dual Attention + BiLSTM'],\n",
                "    'Dataset': ['AffectNet+', 'AffectNet+'],\n",
                "    'Loss Function': ['Standard CE', 'Class-Weighted CE'],\n",
                "    'Accuracy (%)': ['83.50', 'TBD (after training)'],\n",
                "    'F1-Score': ['0.830', 'TBD (after training)'],\n",
                "    'Novelty': ['Cross-domain learning', 'BiLSTM temporal modeling + class weights']\n",
                "}\n",
                "\n",
                "df_comparison = pd.DataFrame(comparison_data)\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"COMPARISON WITH BASE PAPER\")\n",
                "print(\"=\"*100)\n",
                "print(df_comparison.to_string(index=False))\n",
                "print(\"=\"*100)\n",
                "print(\"\\nNote: Update 'TBD' values after completing full training (50 epochs)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "- ✅ Data loading and exploration\n",
                "- ✅ Model architecture (CNN + Dual Attention + BiLSTM)\n",
                "- ✅ Training pipeline (demo with 2 epochs)\n",
                "- ✅ Evaluation metrics and confusion matrix\n",
                "- ✅ Inference on sample images\n",
                "- ✅ Ablation study visualization\n",
                "\n",
                "### Next Steps:\n",
                "1. **Full Training**: Run `python training/train.py --epochs 50` for complete training\n",
                "2. **Evaluation**: Evaluate on test set with `python training/evaluate.py`\n",
                "3. **Ablation Study**: Compare architectures with `python training/ablation_study.py`\n",
                "4. **Real-time Demo**: Test webcam inference with `python inference/webcam_demo.py`\n",
                "\n",
                "### Publication Checklist:\n",
                "- [ ] Train model to 85%+ accuracy\n",
                "- [ ] Complete ablation study\n",
                "- [ ] Generate all visualizations (confusion matrix, ROC curves)\n",
                "- [ ] Document results in comparison table\n",
                "- [ ] Test real-time webcam demo\n",
                "- [ ] Write paper with methodology and results"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}