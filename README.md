# Deep Learning Based Emotion Recognition System

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.1+](https://img.shields.io/badge/pytorch-2.1+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A state-of-the-art emotion recognition system combining **Convolutional Neural Networks (CNN)**, **Dual Attention mechanisms**, and **Bidirectional LSTM** for robust facial expression recognition. This implementation is based on the DCD-DAN (2025) paper with novel architectural enhancements.

## ğŸ¯ Key Features

- **Hybrid Architecture**: CNN + Dual Attention (Channel + Spatial) + BiLSTM
- **High Accuracy**: Targeting 94%+ on RAF-DB dataset
- **7-8 Emotion Classes**: Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger (optional Contempt)
- **Real-time Inference**: Webcam-based emotion recognition
- **Class-weighted Loss**: Handles imbalanced datasets effectively
- **Comprehensive Evaluation**: Confusion matrix, ROC curves, per-class metrics
- **Ablation Study**: Validates each architectural component

## ğŸ“Š Architecture Overview

```
Input Image (224Ã—224)
      â†“
EfficientNetB4/ResNet50 (CNN Backbone)
      â†“
Dual Attention Module
  â”œâ”€ Channel Attention (inter-channel relationships)
  â””â”€ Spatial Attention (important spatial regions)
      â†“
BiLSTM Layer (temporal modeling)
      â†“
Fully Connected Layers + Dropout
      â†“
Softmax (8 emotion classes)
```

## ğŸš€ Installation

### Prerequisites
- Python 3.9+
- CUDA-compatible GPU (recommended, 8GB+ VRAM)
- 20GB+ disk space

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd emotion_recognition_system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“ Project Structure

```
emotion_recognition_system/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/                      # RAF-DB dataset (download separately)
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cnn_dual_attention_bilstm.py  # Model architectures
â”‚   â””â”€â”€ model.py                       # Model utilities
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                       # Training script
â”‚   â”œâ”€â”€ evaluate.py                    # Evaluation script
â”‚   â””â”€â”€ ablation_study.py              # Ablation experiments
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ predict_single.py              # Single image prediction
â”‚   â””â”€â”€ webcam_demo.py                 # Real-time webcam demo
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py                 # Dataset and data loaders
â”‚   â”œâ”€â”€ dual_attention.py              # Attention mechanisms
â”‚   â””â”€â”€ metrics.py                     # Evaluation metrics
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ checkpoints/                   # Trained models
â”‚   â”œâ”€â”€ logs/                          # Training logs
â”‚   â”œâ”€â”€ visualizations/                # Plots and figures
â”‚   â””â”€â”€ ablation/                      # Ablation study results
â””â”€â”€ notebooks/
    â””â”€â”€ full_pipeline.ipynb            # Complete demonstration
```

## ğŸ“¥ Dataset Preparation

### Download RAF-DB Dataset

1. Download RAF-DB from the official source or a verified mirror.
2. Extract the dataset into the `data/` directory.

### Expected Directory Structure

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ 0_neutral/
â”‚   â”œâ”€â”€ 1_happy/
â”‚   â”œâ”€â”€ 2_sad/
â”‚   â”œâ”€â”€ 3_surprise/
â”‚   â”œâ”€â”€ 4_fear/
â”‚   â”œâ”€â”€ 5_disgust/
â”‚   â”œâ”€â”€ 6_anger/
â”‚   â””â”€â”€ 7_contempt/
â”œâ”€â”€ val/
â”‚   â””â”€â”€ [same structure]
â””â”€â”€ test/
    â””â”€â”€ [same structure]
```

## ğŸ“ Training

### Basic Training

```bash
cd training
python train.py --data_dir ../data --epochs 50 --batch_size 32
```

### â˜ï¸ Google Colab Training

We provide a ready-to-use notebook for training on Google Colab (Free or Pro) using your Google Drive for dataset storage.

1. Open `notebooks/colab_training.ipynb` in [Google Colab](https://colab.research.google.com/).
2. Follow the instructions to mount your Drive and start training.


### Advanced Training Options

```bash
python train.py \
    --data_dir ../data \
    --model_type full \
    --backbone efficientnet_b4 \
    --epochs 50 \
    --batch_size 32 \
    --learning_rate 1e-4 \
    --lstm_hidden 256 \
    --lstm_layers 2 \
    --dropout 0.5 \
    --use_class_weights \
    --checkpoint_dir ../results/checkpoints \
    --log_dir ../results/logs
```

### Monitor Training with TensorBoard

```bash
tensorboard --logdir results/logs
```

## ğŸ“Š Evaluation

```bash
cd training
python evaluate.py \
    --checkpoint_path ../results/checkpoints/best_model.pth \
    --data_dir ../data \
    --output_dir ../results/visualizations
```

**Outputs:**
- Confusion matrix (normalized and unnormalized)
- ROC curves for all classes
- Per-class precision, recall, F1-score
- Classification report (CSV)

## ğŸ”¬ Ablation Study

Compare different architectural variants:

```bash
cd training
python ablation_study.py \
    --data_dir ../data \
    --epochs 20 \
    --output_dir ../results/ablation
```

**Tested Configurations:**
1. Baseline CNN (EfficientNetB4 only)
2. CNN + Dual Attention (no BiLSTM)
3. Full Model (CNN + Dual Attention + BiLSTM + class weights)

## ğŸ¬ Inference

### Single Image Prediction

```bash
cd inference
python predict_single.py \
    --image_path /path/to/image.jpg \
    --model_path ../results/checkpoints/best_model.pth \
    --visualize
```

### Real-time Webcam Demo

```bash
cd inference
python webcam_demo.py \
    --model_path ../results/checkpoints/best_model.pth \
    --camera_id 0
```

**Controls:**
- Press `q` to quit
- Optional: Save video with `--output_video output.mp4`

## ğŸ“ˆ Results

### Expected Performance (AffectNet+ dataset)

| Metric | Target | Achieved |
|--------|--------|----------|
| Accuracy | 85%+ | TBD after training |
| Macro F1-Score | 0.82+ | TBD after training |
| Inference Speed | 30+ FPS | ~40 FPS (GPU) |

### Comparison with Base Paper

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| DCD-DAN (2025) | 83.5% | 0.831 | 0.829 | 0.830 |
| **Our Model** | **TBD** | **TBD** | **TBD** | **TBD** |

*Table will be populated after training*

## ğŸ”§ Model Architecture Details

### CNN Backbone Options
- **EfficientNetB4** (default): 19M parameters, excellent accuracy/efficiency trade-off
- **ResNet50**: 25M parameters, robust feature extraction

### Dual Attention Mechanism
- **Channel Attention**: Learns "what" is important (inter-channel relationships)
- **Spatial Attention**: Learns "where" is important (spatial locations)
- **Reduction Ratio**: 16 (balances performance and computation)

### BiLSTM Configuration
- **Hidden Size**: 256
- **Layers**: 2
- **Bidirectional**: Yes (captures past and future context)

### Training Hyperparameters
- **Optimizer**: AdamW (lr=1e-4, weight_decay=1e-5)
- **Scheduler**: ReduceLROnPlateau (factor=0.5, patience=5)
- **Loss**: Class-weighted CrossEntropy
- **Batch Size**: 32
- **Early Stopping**: Patience 10 epochs

## ğŸ“š Citation

If you use this code in your research, please cite:

**Base Paper:**
```bibtex
@article{dcd-dan-2025,
  title={A novel facial expression recognition framework using deep learning based dynamic cross-domain dual attention network},
  year={2025},
  journal={[Journal Name]},
  author={[Authors]}
}
```

**This Implementation:**
```bibtex
@misc{emotion-recognition-2025,
  title={Deep Learning Based Emotion Recognition System},
  author={[Your Name]},
  year={2025},
  publisher={GitHub},
  howpublished={\url{[repository-url]}}
}
```

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- **DCD-DAN Paper** (2025) for the foundational architecture
- **AffectNet+** dataset creators
- **PyTorch** team for the deep learning framework
- **timm** library for pretrained models

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub or contact [your-email@example.com].

## ğŸ› Known Issues & Troubleshooting

### Issue: CUDA Out of Memory
**Solution**: Reduce batch size or use mixed precision training:
```bash
python train.py --batch_size 16
```

### Issue: No face detected in webcam
**Solution**: Ensure good lighting and face the camera directly. Adjust `minNeighbors` parameter in face detection.

### Issue: Low accuracy on validation set
**Solution**: 
- Ensure dataset is properly balanced
- Increase training epochs
- Try different backbone (ResNet50 vs EfficientNetB4)
- Verify data augmentation settings

## ğŸ¯ Future Work

- [ ] Add support for video emotion recognition
- [ ] Implement attention visualization
- [ ] Export model to ONNX/TensorRT for faster inference
- [ ] Add support for wild (unconstrained) facial images
- [ ] Multi-task learning (emotion + age + gender)
- [ ] Mobile deployment (TensorFlow Lite)

---

**Built with â¤ï¸ for advancing emotion AI research**
